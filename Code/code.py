# -*- coding: utf-8 -*-
"""Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c5TUqkmKSrkmETw6IT4F1s7JQNmi376U
"""

from google.colab import drive
import os

# Mount your Google Drive
drive.mount('/content/drive')

# Define the dataset folder path
dataset_path = "/content/BDWaste"  # We'll use this as the main working directory

# If your dataset is in Drive, extract or copy it to /content/BDWaste
# Example: zip_path = "/content/drive/MyDrive/bdwaste.zip"

import zipfile
import shutil

# Path to ZIP file in Drive (edit this if you‚Äôre using a ZIP)
zip_path = "/content/drive/MyDrive/BDWaste.zip"

# Extract if ZIP file exists
if os.path.exists(zip_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall("/content/")
    print("‚úÖ Dataset extracted to /content/")
else:
    print("‚ö†Ô∏è ZIP file not found at the path provided.")

# OR if dataset is already a folder in Drive
# drive_folder_path = "/content/drive/MyDrive/BDWaste"
# shutil.copytree(drive_folder_path, dataset_path, dirs_exist_ok=True)

import matplotlib.pyplot as plt
import seaborn as sns
from glob import glob

# Define paths for Digestive and Indigestive folders
digestive_path = os.path.join(dataset_path, "Digestive")
indigestive_path = os.path.join(dataset_path, "Indigestive")

# Function to count total images
def count_images(folder):
    counts = {}
    for category in os.listdir(folder):
        path = os.path.join(folder, category)
        if os.path.isdir(path):
            counts[category] = len(glob(os.path.join(path, "*.*")))
    return counts

digestive_counts = count_images(digestive_path)
indigestive_counts = count_images(indigestive_path)
total_counts = {**digestive_counts, **indigestive_counts}

# Plot
plt.figure(figsize=(12, 5))
sns.barplot(x=list(total_counts.keys()), y=list(total_counts.values()), palette="coolwarm")
plt.xticks(rotation=90)
plt.xlabel("Waste Categories")
plt.ylabel("Image Count")
plt.title("Image Count per Category in BDWaste Dataset")
plt.show()

# Print counts
print("\n‚úÖ Category-wise Image Count:")
for category, count in total_counts.items():
    print(f"üìå {category}: {count} images")

import os
import shutil
import random

# Original dataset path
original_path = "/content/BDWaste"

# New root for split dataset
split_path = "/content/BDWaste_Split"
train_path = os.path.join(split_path, "train")
val_path = os.path.join(split_path, "val")
test_path = os.path.join(split_path, "test")

# Function to split each category
def split_data(source_folder, train_folder, val_folder, test_folder, train_ratio=0.8, val_ratio=0.1):
    for category in os.listdir(source_folder):
        category_path = os.path.join(source_folder, category)
        if os.path.isdir(category_path):
            images = os.listdir(category_path)
            random.shuffle(images)

            total = len(images)
            train_end = int(total * train_ratio)
            val_end = train_end + int(total * val_ratio)

            splits = {
                train_folder: images[:train_end],
                val_folder: images[train_end:val_end],
                test_folder: images[val_end:]
            }

            for split_folder, files in splits.items():
                cat_dir = os.path.join(split_folder, category)
                os.makedirs(cat_dir, exist_ok=True)
                for file in files:
                    src = os.path.join(category_path, file)
                    dst = os.path.join(cat_dir, file)
                    shutil.copyfile(src, dst)

# Run the split for Digestive and Indigestive
for class_type in ["Digestive", "Indigestive"]:
    src = os.path.join(original_path, class_type)
    split_data(
        src,
        os.path.join(train_path, class_type),
        os.path.join(val_path, class_type),
        os.path.join(test_path, class_type)
    )

print("‚úÖ Dataset split into train (80%), val (10%), and test (10%)")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Image properties
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32

# Augmentation for training only
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    brightness_range=[0.8, 1.2],
    horizontal_flip=True
)

# No augmentation for val/test ‚Äî only normalization
val_test_datagen = ImageDataGenerator(rescale=1./255)

# Load images
train_generator = train_datagen.flow_from_directory(
    train_path,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

val_generator = val_test_datagen.flow_from_directory(
    val_path,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

test_generator = val_test_datagen.flow_from_directory(
    test_path,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False  # Important for test evaluation
)

print("‚úÖ Data generators ready: train, validation, test")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

# Define the model (same structure, clean input)
model = Sequential([
    Input(shape=(224, 224, 3)),  # ‚úÖ Replaces input_shape in Conv2D

    # Block 1
    Conv2D(32, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    # Block 2
    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    # Block 3
    Conv2D(128, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    # Classifier
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')  # Output layer
])

# Compile the model
model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

# Model summary
model.summary()

# Train the model
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10,
    verbose=1
)

import matplotlib.pyplot as plt

# Plot accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training vs Validation Accuracy')
plt.legend()
plt.show()

# Plot loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss')
plt.legend()
plt.show()

# Evaluate on test set
test_loss, test_acc = model.evaluate(test_generator)
print(f"\nüéØ Test Accuracy: {test_acc:.4f}")
print(f"üß™ Test Loss: {test_loss:.4f}")

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Get predictions
Y_pred = model.predict(test_generator)
y_pred = np.argmax(Y_pred, axis=1)

# True labels
y_true = test_generator.classes
class_labels = list(test_generator.class_indices.keys())

# Classification report
print("üìä Classification Report:\n")
print(classification_report(y_true, y_pred, target_names=class_labels))

# Confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot
plt.figure(figsize=(12, 8))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels, cmap="Blues")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

import tensorflow as tf
from tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB0, MobileNetV2
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# Dictionary of model configs
architectures = {
    "VGG16": VGG16,
    "ResNet50": ResNet50,
    "EfficientNetB0": EfficientNetB0,
    "MobileNetV2": MobileNetV2
}

results = {}

for name, base_fn in architectures.items():
    print(f"\nüîß Training model: {name}")

    # Load base model without top layers
    base_model = base_fn(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    # Freeze base layers
    for layer in base_model.layers:
        layer.trainable = False

    # Add custom head
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)
    predictions = Dense(train_generator.num_classes, activation='softmax')(x)

    # Build full model
    model = Model(inputs=base_model.input, outputs=predictions)

    # Compile
    model.compile(optimizer=Adam(learning_rate=1e-4),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    # Train
    history = model.fit(
        train_generator,
        validation_data=val_generator,
        epochs=10,
        callbacks=[EarlyStopping(patience=3, restore_best_weights=True)],
        verbose=1
    )

    # Save results
    val_acc = history.history['val_accuracy'][-1]
    results[name] = val_acc

    # Save model
    model.save(f"{name}_waste_classifier.h5")

    # Plot accuracy curve
    plt.plot(history.history['accuracy'], label=f'{name} Train')
    plt.plot(history.history['val_accuracy'], label=f'{name} Val')
    plt.title(f'{name} Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# Print comparison
print("\nüìä Model Validation Accuracies:")
for model_name, acc in results.items():
    print(f"{model_name}: {acc:.4f}")

# STEP 1: Imports
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import load_model

# STEP 2: Load Test Generator
test_path = "/content/BDWaste_Split/test"  # Update if needed

test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    test_path,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

# STEP 3: Load Pretrained Models (if already trained and saved)
vgg_model = load_model("VGG16_waste_classifier.h5")
resnet_model = load_model("ResNet50_waste_classifier.h5")
efficientnet_model = load_model("EfficientNetB0_waste_classifier.h5")
mobilenet_model = load_model("MobileNetV2_waste_classifier.h5")

# STEP 4: Evaluation Function
def evaluate_model_on_test(model, model_name):
    print(f"\nüîç Evaluating {model_name} on Test Set")

    # Test accuracy & loss
    test_loss, test_acc = model.evaluate(test_generator, verbose=1)
    print(f"\n‚úÖ {model_name} - Test Accuracy: {test_acc:.4f}")
    print(f"üß™ {model_name} - Test Loss: {test_loss:.4f}")

    # Predictions
    predictions = model.predict(test_generator)
    y_pred = np.argmax(predictions, axis=1)
    y_true = test_generator.classes
    class_labels = list(test_generator.class_indices.keys())

    # Classification report
    print(f"\nüìä {model_name} - Classification Report")
    print(classification_report(y_true, y_pred, target_names=class_labels))

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_labels, yticklabels=class_labels)
    plt.title(f'{model_name} - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# STEP 5: Evaluate All Models
evaluate_model_on_test(vgg_model, "VGG16")
evaluate_model_on_test(resnet_model, "ResNet50")
evaluate_model_on_test(efficientnet_model, "EfficientNetB0")
evaluate_model_on_test(mobilenet_model, "MobileNetV2")

import os

base_path = "/content"

# List all folders in /content
for folder in os.listdir(base_path):
    print("üìÅ", folder)